{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2dea6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\u1105800\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "import numpy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3d229fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Summary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\")\\\n",
    "          .appName(\"SparkByExamples.com\")\\\n",
    "          .getOrCreate()\n",
    "df = spark.read.csv(\"C:/Users/u1105800/PG/Capstone/Capstone/capstone/Capstone/sample_dataset.csv\",header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4407c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Category|             Summary|\n",
      "+--------+--------------------+\n",
      "|Business|Reuters - Short-s...|\n",
      "|Business|Reuters - Private...|\n",
      "|Business|Reuters - Soaring...|\n",
      "|Business|Reuters - Authori...|\n",
      "|Business|AFP - Tearaway wo...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9fda61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Category|count|\n",
      "+--------------------+-----+\n",
      "|  Science/Technology|35334|\n",
      "|            Business|33992|\n",
      "|              Sports|32757|\n",
      "|               World|31900|\n",
      "|       Entertainment| 2583|\n",
      "|            Politics| 2377|\n",
      "|This story has be...|  125|\n",
      "|             However|   82|\n",
      "|           Meanwhile|   27|\n",
      "|             Further|   18|\n",
      "|             In 2014|   17|\n",
      "|    So far this year|   16|\n",
      "|                Also|   15|\n",
      "|          To be sure|   15|\n",
      "|             However|   14|\n",
      "|               Still|   14|\n",
      "|             Besides|   13|\n",
      "|                 Now|   12|\n",
      "|           On Monday|   11|\n",
      "|           At 9.15am|   10|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.groupBy(\"Category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b51cb211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107043"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter((df.Category == 'Business') | (df.Category == 'Sports') | (df.Category == 'Entertainment')|(df.Category == 'Politics')|(df.Category == 'Science/Technology')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a7f77d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152599\n",
      "149881\n"
     ]
    }
   ],
   "source": [
    "print(df.count())\n",
    "df = df.dropna()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d25bcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleansing done!!\n",
      "Columns after data cleansing:  ['Summary', 'Category', 'words', 'filtered_data', 'tf', 'idf', 'label']\n",
      "Schema after data cleansing\n",
      "root\n",
      " |-- Summary: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- idf: vector (nullable = true)\n",
      " |-- label: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df['Summary','Category']\n",
    "\n",
    "stopwords_ = stopwords.words('english')\n",
    "tokenizer = Tokenizer(inputCol=\"Summary\", outputCol=\"words\")\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_data\").setStopWords(stopwords_)\n",
    "hashingTF = HashingTF(inputCol=\"filtered_data\", outputCol=\"tf\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"idf\", minDocFreq=5)\n",
    "labelAnnotator = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "preprocessorPipeline = Pipeline(stages=[tokenizer, stopwordsRemover, hashingTF, idf, labelAnnotator])\n",
    "preprocessorPipelineFit = preprocessorPipeline.fit(df)\n",
    "\n",
    "#preprocessorPipelineFit.save('preprocessor')\n",
    "\n",
    "#preprocessor = PipelineModel.load(\"preprocessor\")\n",
    "\n",
    "#cleaned_df = preprocessorPipelineFit.transform(df)\n",
    "cleaned_df = preprocessorPipelineFit.transform(df)\n",
    "\n",
    "print('Data cleansing done!!')\n",
    "\n",
    "print('Columns after data cleansing: ',cleaned_df.columns)\n",
    "\n",
    "cleaned_df = cleaned_df['Summary', 'Category', 'tf', 'idf', 'label']\n",
    "\n",
    "print('Schema after data cleansing')\n",
    "cleaned_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "480e3e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|             Summary|Category|                  tf|                 idf|label|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "|Reuters - Short-s...|Business|(10000,[551,1152,...|(10000,[551,1152,...|  1.0|\n",
      "|Reuters - Private...|Business|(10000,[1152,1562...|(10000,[1152,1562...|  1.0|\n",
      "|Reuters - Soaring...|Business|(10000,[217,532,7...|(10000,[217,532,7...|  1.0|\n",
      "+--------------------+--------+--------------------+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9f412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
